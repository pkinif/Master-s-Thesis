---
title: "ModelAnalysis"
author: "Kinif Pierrick"
date: "26 avril 2018"
output: pdf_document
---
```{r echo = FALSE, message = FALSE, error = FALSE, results = 'asis'}

if (!require("Amelia")) install.packages("Amelia")
if (!require("data.table")) install.packages("data.table")
if (!require("dplyr")) install.packages("dplyr")

library(Amelia) # to handle na's with mutliple imputation
library(data.table)
library(dplyr)

#####################################
# Analysis Model
#####################################

# Based on https://www.andrewheiss.com/blog/2018/03/07/amelia-tidy-melding/

# a.out is a list of data frames, adn each imputed dataset is stored in a list slot named "imputations" or a.out$imputations. Let's combine these all into one big data frame with bind_rows(), group by the immputation number(i.e. m) and nest them into imputation specific rows :

require(tidyverse)
require(plm)
require(broom)
require(stats)
  require(purrr)


######################################################
######################TobinsQ#########################
######################################################

#####################
# Remove Outliers
#####################

# I remove outliers which are considered as influencials. For this I create a vector from 1:100, namely the number of iteration. I create a vector FileList which contains the libellé of each csv file and I make a function to remove outliers on each csvfile.

i <- c(1:10)

FileList <- lapply(i, function(x){
  paste("outdata", x , ".csv", sep = "")
})


NoOutliers <- lapply(FileList,function(x){
  
  #Create the path file
  path <- paste("DataBase/ImputationDataBase/Lag0/", x, sep = "")
  
  #For each File I remove outliers and save into a new variable
  GetNewFiles <- function(path){
  
    file <- read.csv(file = path, header = TRUE, stringsAsFactors = FALSE)
  
    Roa_Lm <- lm(LogTobinsQ ~ SustainabilityPayLink + SustainableThemedCommitment + AuditScore + CarbonProductivity + WaterProductivity + WasteProductivity + DebtToEquityRatio + NetMargin + FirmSize + Beta + GicsClassification , data = file)
  
    cooksdRoa <- cooks.distance(Roa_Lm)
    influentialRoa <- as.numeric(names(cooksdRoa)[(cooksdRoa > 4*mean(cooksdRoa, na.rm=T))])
    NewFiles <- file[-c(influentialRoa),]
  
  }
  
  #Map everything into a dataframe
  map_df(path, GetNewFiles) -> results
  results$Imputation <- x
  return(results)
  
})

#Make the right format

NoOutliersTibble <- bind_rows(unclass(NoOutliers)) %>%
  group_by(Imputation) %>%
  nest() %>%
  as_tibble()


######################
# Regression Analysis
######################



# With this nested data, let's use purr :: map() to run models and return tidy summaries of those models directly in the data frame


models_imputations <- NoOutliersTibble %>%
  mutate(model = data %>% map(~ plm(LogTobinsQ ~ SustainabilityPayLink + SustainableThemedCommitment + AuditScore + CarbonProductivity + WaterProductivity + WasteProductivity + DebtToEquityRatio + NetMargin + FirmSize + Beta + GicsClassification , index = c("Index", "Year"), model = "random", data = .)),
         tidied = model %>% map(~ tidy(., conf.int = TRUE)),
         glance = model %>% map(~ glance(.)))

# Having the models structured like this makes it easy to access coefficients for models from individual imputations, like so:

Coefficient_Outdata1.csv <- models_imputations %>%
  filter(Imputation == "outdata8.csv") %>%
  unnest(tidied)

# Create a wide data frame of just the coefficients and standard errors
params <- models_imputations %>%
  unnest(tidied) %>%
  select(Imputation, term, estimate, std.error) %>%
  gather(key, value, estimate, std.error) %>%
  spread(term, value)

# Extract just the coefficients
just_coefs <- params %>%
  filter(key == "estimate") %>%
  select(-Imputation, -key)

# Extract just the standard errors
just_ses <- params %>%
  filter(key == "std.error") %>%
  select(-Imputation, -key)

# then use these matrices in mi.meld(), which returns a list with two slots—q.mi and se.mi:
coefs_melded <- mi.meld(just_coefs, just_ses)

#Armed with these, let's create the regression summary table with some more dplyr wizardry. To calculate the p-value and confidence intervals, I need to extract the degrees of freedom from one of the imputed models

model_degree_freedom <- models_imputations %>%
  unnest(glance) %>%
  filter(Imputation == "outdata8.csv") %>%
  pull(df.residual)

melded_summary <- as.data.frame(cbind(t(coefs_melded$q.mi),
                                      t(coefs_melded$se.mi))) %>%
  magrittr::set_colnames(c("estimate", "std.error")) %>%
  mutate(term = rownames(.)) %>%
  select(term, everything()) %>%
  mutate(statistic = estimate / std.error,
         conf.low = estimate + std.error * qt(0.025, model_degree_freedom),
         conf.high = estimate + std.error * qt(0.975, model_degree_freedom),
         p.value = 2 * pt(abs(statistic), model_degree_freedom, lower.tail = FALSE))


# Let's add the R^2 and F statistic - It can be done in two steps :

# Step 1: in each complete data set, calculate R2, take its square root,
# transform it with Fisher z-transformation, and calculate the variance of R2\
r2s <- models_imputations %>%
  unnest(glance) %>%
  select(Imputation, adj.r.squared, df.residual) %>%
  mutate(R = sqrt(adj.r.squared),  # Regular R
         Q = 0.5 * log((R + 1) / (1 - R)),  # Fisher z-transformation
         se = 1 / df.residual)  # R2 variance

#Step 2: combine the results using Rubin's rules (mi.meld()), inverse transform the value, and square it

# Meld the R2 values with mi.meld()
Q_melded <- mi.meld(as.matrix(r2s$Q), as.matrix(r2s$se))

# Inverse transform Q to R and square it
r2_melded <- ((exp(2 * Q_melded$q.mi) - 1) / (1 + exp(2 * Q_melded$q.mi)))^2


#####################################
#### Put the model into a table #####
#####################################

require(xtable)

test <- xtable(melded_summary, caption = "LogTobinsQ No Outliers", label = "TobinsQ")
print.xtable(test)



########################################################
####################Roa#################################
########################################################



#####################
# Remove Outliers
#####################

# I remove outliers which are considered as influencials. For this I create a vector from 1:100, namely the number of iteration. I create a vector FileList which contains the libellé of each csv file and I make a function to remove outliers on each csvfile.

i <- c(1:10)

FileList <- lapply(i, function(x){
  paste("outdata", x , ".csv", sep = "")
})


NoOutliers <- lapply(FileList,function(x){
  
  #Create the path file
  path <- paste("DataBase/ImputationDataBase/Lag0/", x, sep = "")
  
  #For each File I remove outliers and save into a new variable
  GetNewFiles <- function(path){
  
    file <- read.csv(file = path, header = TRUE, stringsAsFactors = FALSE)
  
    Roa_Lm <- lm(Roa ~ SustainabilityPayLink + SustainableThemedCommitment + AuditScore + CarbonProductivity + WaterProductivity + WasteProductivity + DebtToEquityRatio + NetMargin + FirmSize + Beta + GicsClassification , data = file)
  
    cooksdRoa <- cooks.distance(Roa_Lm)
    influentialRoa <- as.numeric(names(cooksdRoa)[(cooksdRoa > 4*mean(cooksdRoa, na.rm=T))])
    NewFiles <- file[-c(influentialRoa),]
  
  }
  
  #Map everything into a dataframe
  map_df(path, GetNewFiles) -> results
  results$Imputation <- x
  return(results)
  
})

#Make the right format

NoOutliersTibble <- bind_rows(unclass(NoOutliers)) %>%
  group_by(Imputation) %>%
  nest() %>%
  as_tibble()


######################
# Regression Analysis
######################



# With this nested data, let's use purr :: map() to run models and return tidy summaries of those models directly in the data frame


models_imputations <- NoOutliersTibble %>%
  mutate(model = data %>% map(~ plm(Roa ~ SustainabilityPayLink + SustainableThemedCommitment + AuditScore + CarbonProductivity + WaterProductivity + WasteProductivity + DebtToEquityRatio + NetMargin + FirmSize + Beta + GicsClassification , index = c("Index", "Year"), model = "random", data = .)),
         tidied = model %>% map(~ tidy(., conf.int = TRUE)),
         glance = model %>% map(~ glance(.)))

# Having the models structured like this makes it easy to access coefficients for models from individual imputations, like so:

Coefficient_Outdata1.csv <- models_imputations %>%
  filter(Imputation == "outdata8.csv") %>%
  unnest(tidied)

# Create a wide data frame of just the coefficients and standard errors
params <- models_imputations %>%
  unnest(tidied) %>%
  select(Imputation, term, estimate, std.error) %>%
  gather(key, value, estimate, std.error) %>%
  spread(term, value)

# Extract just the coefficients
just_coefs <- params %>%
  filter(key == "estimate") %>%
  select(-Imputation, -key)

# Extract just the standard errors
just_ses <- params %>%
  filter(key == "std.error") %>%
  select(-Imputation, -key)

# then use these matrices in mi.meld(), which returns a list with two slots—q.mi and se.mi:
coefs_melded <- mi.meld(just_coefs, just_ses)

#Armed with these, let's create the regression summary table with some more dplyr wizardry. To calculate the p-value and confidence intervals, I need to extract the degrees of freedom from one of the imputed models

model_degree_freedom <- models_imputations %>%
  unnest(glance) %>%
  filter(Imputation == "outdata8.csv") %>%
  pull(df.residual)

melded_summary <- as.data.frame(cbind(t(coefs_melded$q.mi),
                                      t(coefs_melded$se.mi))) %>%
  magrittr::set_colnames(c("estimate", "std.error")) %>%
  mutate(term = rownames(.)) %>%
  select(term, everything()) %>%
  mutate(statistic = estimate / std.error,
         conf.low = estimate + std.error * qt(0.025, model_degree_freedom),
         conf.high = estimate + std.error * qt(0.975, model_degree_freedom),
         p.value = 2 * pt(abs(statistic), model_degree_freedom, lower.tail = FALSE))


# Let's add the R^2 and F statistic - It can be done in two steps :

# Step 1: in each complete data set, calculate R2, take its square root,
# transform it with Fisher z-transformation, and calculate the variance of R2\
r2s <- models_imputations %>%
  unnest(glance) %>%
  select(Imputation, adj.r.squared, df.residual) %>%
  mutate(R = sqrt(adj.r.squared),  # Regular R
         Q = 0.5 * log((R + 1) / (1 - R)),  # Fisher z-transformation
         se = 1 / df.residual)  # R2 variance

#Step 2: combine the results using Rubin's rules (mi.meld()), inverse transform the value, and square it

# Meld the R2 values with mi.meld()
Q_melded <- mi.meld(as.matrix(r2s$Q), as.matrix(r2s$se))

# Inverse transform Q to R and square it
r2_melded <- ((exp(2 * Q_melded$q.mi) - 1) / (1 + exp(2 * Q_melded$q.mi)))^2


#####################################
#### Put the model into a table #####
#####################################

require(xtable)
require(stargazer)
test <- xtable(melded_summary, caption = "Roa No Outliers", label = "Roa")
print.xtable(test)

########################################################
####################Roe#################################
########################################################



#####################
# Remove Outliers
#####################

# I remove outliers which are considered as influencials. For this I create a vector from 1:100, namely the number of iteration. I create a vector FileList which contains the libellé of each csv file and I make a function to remove outliers on each csvfile.

i <- c(1:10)

FileList <- lapply(i, function(x){
  paste("outdata", x , ".csv", sep = "")
})


NoOutliers <- lapply(FileList,function(x){
  
  #Create the path file
  path <- paste("DataBase/ImputationDataBase/Lag0/", x, sep = "")
  
  #For each File I remove outliers and save into a new variable
  GetNewFiles <- function(path){
  
    file <- read.csv(file = path, header = TRUE, stringsAsFactors = FALSE)
  
    Roa_Lm <- lm(Roe ~ SustainabilityPayLink + SustainableThemedCommitment + AuditScore + CarbonProductivity + WaterProductivity + WasteProductivity + DebtToEquityRatio + NetMargin + FirmSize + Beta + GicsClassification , data = file)
  
    cooksdRoa <- cooks.distance(Roa_Lm)
    influentialRoa <- as.numeric(names(cooksdRoa)[(cooksdRoa > 4*mean(cooksdRoa, na.rm=T))])
    NewFiles <- file[-c(influentialRoa),]
  
  }
  
  #Map everything into a dataframe
  map_df(path, GetNewFiles) -> results
  results$Imputation <- x
  return(results)
  
})

#Make the right format

NoOutliersTibble <- bind_rows(unclass(NoOutliers)) %>%
  group_by(Imputation) %>%
  nest() %>%
  as_tibble()


######################
# Regression Analysis
######################



# With this nested data, let's use purr :: map() to run models and return tidy summaries of those models directly in the data frame


models_imputations <- NoOutliersTibble %>%
  mutate(model = data %>% map(~ plm(Roe ~ SustainabilityPayLink + SustainableThemedCommitment + AuditScore + CarbonProductivity + WaterProductivity + WasteProductivity + DebtToEquityRatio + NetMargin + FirmSize + Beta + GicsClassification , index = c("Index", "Year"), model = "random", data = .)),
         tidied = model %>% map(~ tidy(., conf.int = TRUE)),
         glance = model %>% map(~ glance(.)))

# Having the models structured like this makes it easy to access coefficients for models from individual imputations, like so:

Coefficient_Outdata1.csv <- models_imputations %>%
  filter(Imputation == "outdata8.csv") %>%
  unnest(tidied)

# Create a wide data frame of just the coefficients and standard errors
params <- models_imputations %>%
  unnest(tidied) %>%
  select(Imputation, term, estimate, std.error) %>%
  gather(key, value, estimate, std.error) %>%
  spread(term, value)

# Extract just the coefficients
just_coefs <- params %>%
  filter(key == "estimate") %>%
  select(-Imputation, -key)

# Extract just the standard errors
just_ses <- params %>%
  filter(key == "std.error") %>%
  select(-Imputation, -key)

# then use these matrices in mi.meld(), which returns a list with two slots—q.mi and se.mi:
coefs_melded <- mi.meld(just_coefs, just_ses)

#Armed with these, let's create the regression summary table with some more dplyr wizardry. To calculate the p-value and confidence intervals, I need to extract the degrees of freedom from one of the imputed models

model_degree_freedom <- models_imputations %>%
  unnest(glance) %>%
  filter(Imputation == "outdata8.csv") %>%
  pull(df.residual)

melded_summary <- as.data.frame(cbind(t(coefs_melded$q.mi),
                                      t(coefs_melded$se.mi))) %>%
  magrittr::set_colnames(c("estimate", "std.error")) %>%
  mutate(term = rownames(.)) %>%
  select(term, everything()) %>%
  mutate(statistic = estimate / std.error,
         conf.low = estimate + std.error * qt(0.025, model_degree_freedom),
         conf.high = estimate + std.error * qt(0.975, model_degree_freedom),
         p.value = 2 * pt(abs(statistic), model_degree_freedom, lower.tail = FALSE))


# Let's add the R^2 and F statistic - It can be done in two steps :

# Step 1: in each complete data set, calculate R2, take its square root,
# transform it with Fisher z-transformation, and calculate the variance of R2\
r2s <- models_imputations %>%
  unnest(glance) %>%
  select(Imputation, adj.r.squared, df.residual) %>%
  mutate(R = sqrt(adj.r.squared),  # Regular R
         Q = 0.5 * log((R + 1) / (1 - R)),  # Fisher z-transformation
         se = 1 / df.residual)  # R2 variance

#Step 2: combine the results using Rubin's rules (mi.meld()), inverse transform the value, and square it

# Meld the R2 values with mi.meld()
Q_melded <- mi.meld(as.matrix(r2s$Q), as.matrix(r2s$se))

# Inverse transform Q to R and square it
r2_melded <- ((exp(2 * Q_melded$q.mi) - 1) / (1 + exp(2 * Q_melded$q.mi)))^2


#####################################
#### Put the model into a table #####
#####################################

require(xtable)
require(stargazer)
test <- xtable(melded_summary, caption = "Roe No Outliers", label = "Roe")
print.xtable(test)

########################################################
####################Roic#################################
########################################################



#####################
# Remove Outliers
#####################

# I remove outliers which are considered as influencials. For this I create a vector from 1:100, namely the number of iteration. I create a vector FileList which contains the libellé of each csv file and I make a function to remove outliers on each csvfile.

i <- c(1:10)

FileList <- lapply(i, function(x){
  paste("outdata", x , ".csv", sep = "")
})


NoOutliers <- lapply(FileList,function(x){
  
  #Create the path file
  path <- paste("DataBase/ImputationDataBase/Lag0/", x, sep = "")
  
  #For each File I remove outliers and save into a new variable
  GetNewFiles <- function(path){
  
    file <- read.csv(file = path, header = TRUE, stringsAsFactors = FALSE)
  
    Roa_Lm <- lm(Roic ~ SustainabilityPayLink + SustainableThemedCommitment + AuditScore + CarbonProductivity + WaterProductivity + WasteProductivity + DebtToEquityRatio + NetMargin + FirmSize + Beta + GicsClassification , data = file)
  
    cooksdRoa <- cooks.distance(Roa_Lm)
    influentialRoa <- as.numeric(names(cooksdRoa)[(cooksdRoa > 4*mean(cooksdRoa, na.rm=T))])
    NewFiles <- file[-c(influentialRoa),]
  
  }
  
  #Map everything into a dataframe
  map_df(path, GetNewFiles) -> results
  results$Imputation <- x
  return(results)
  
})

#Make the right format

NoOutliersTibble <- bind_rows(unclass(NoOutliers)) %>%
  group_by(Imputation) %>%
  nest() %>%
  as_tibble()


######################
# Regression Analysis
######################



# With this nested data, let's use purr :: map() to run models and return tidy summaries of those models directly in the data frame


models_imputations <- NoOutliersTibble %>%
  mutate(model = data %>% map(~ plm(Roic ~ SustainabilityPayLink + SustainableThemedCommitment + AuditScore + CarbonProductivity + WaterProductivity + WasteProductivity + DebtToEquityRatio + NetMargin + FirmSize + Beta + GicsClassification , index = c("Index", "Year"), model = "random", data = .)),
         tidied = model %>% map(~ tidy(., conf.int = TRUE)),
         glance = model %>% map(~ glance(.)))

# Having the models structured like this makes it easy to access coefficients for models from individual imputations, like so:

Coefficient_Outdata1.csv <- models_imputations %>%
  filter(Imputation == "outdata8.csv") %>%
  unnest(tidied)

# Create a wide data frame of just the coefficients and standard errors
params <- models_imputations %>%
  unnest(tidied) %>%
  select(Imputation, term, estimate, std.error) %>%
  gather(key, value, estimate, std.error) %>%
  spread(term, value)

# Extract just the coefficients
just_coefs <- params %>%
  filter(key == "estimate") %>%
  select(-Imputation, -key)

# Extract just the standard errors
just_ses <- params %>%
  filter(key == "std.error") %>%
  select(-Imputation, -key)

# then use these matrices in mi.meld(), which returns a list with two slots—q.mi and se.mi:
coefs_melded <- mi.meld(just_coefs, just_ses)

#Armed with these, let's create the regression summary table with some more dplyr wizardry. To calculate the p-value and confidence intervals, I need to extract the degrees of freedom from one of the imputed models

model_degree_freedom <- models_imputations %>%
  unnest(glance) %>%
  filter(Imputation == "outdata8.csv") %>%
  pull(df.residual)

melded_summary <- as.data.frame(cbind(t(coefs_melded$q.mi),
                                      t(coefs_melded$se.mi))) %>%
  magrittr::set_colnames(c("estimate", "std.error")) %>%
  mutate(term = rownames(.)) %>%
  select(term, everything()) %>%
  mutate(statistic = estimate / std.error,
         conf.low = estimate + std.error * qt(0.025, model_degree_freedom),
         conf.high = estimate + std.error * qt(0.975, model_degree_freedom),
         p.value = 2 * pt(abs(statistic), model_degree_freedom, lower.tail = FALSE))


# Let's add the R^2 and F statistic - It can be done in two steps :

# Step 1: in each complete data set, calculate R2, take its square root,
# transform it with Fisher z-transformation, and calculate the variance of R2\
r2s <- models_imputations %>%
  unnest(glance) %>%
  select(Imputation, adj.r.squared, df.residual) %>%
  mutate(R = sqrt(adj.r.squared),  # Regular R
         Q = 0.5 * log((R + 1) / (1 - R)),  # Fisher z-transformation
         se = 1 / df.residual)  # R2 variance

#Step 2: combine the results using Rubin's rules (mi.meld()), inverse transform the value, and square it

# Meld the R2 values with mi.meld()
Q_melded <- mi.meld(as.matrix(r2s$Q), as.matrix(r2s$se))

# Inverse transform Q to R and square it
r2_melded <- ((exp(2 * Q_melded$q.mi) - 1) / (1 + exp(2 * Q_melded$q.mi)))^2


#####################################
#### Put the model into a table #####
#####################################

require(xtable)
require(stargazer)
test <- xtable(melded_summary, caption = "Roic No Outliers", label = "Roic")
print.xtable(test)
```

